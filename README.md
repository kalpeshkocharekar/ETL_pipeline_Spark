# ETL_pipeline_Spark
Create an ETL pipeline using Spark to analyze data and store it into PostgreSQL

Here we will be running the spark job to extract data from the JSON files and transform into dataframes and store it into the database.
you will need a Postgresql up and running with username etl.
Just need to specify the same in the etl.py file

below are the file descriptions

[etl.py]
