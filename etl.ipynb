{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:42.1.1 pyspark-shell'\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have imported the dependecies required to run the pyspark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"first app\")\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Spark ETL Pipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " create a sparkcontext object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdata = spark.read.json(\"jsondata/log_data/*/*/*.json\")\n",
    "songdata = spark.read.json(\"jsondata/song_data/*/*/*/*.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have loaded the data from the respective folders into the Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process Songdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data = songdata[['song_id','title','artist_id', 'year', 'duration']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the required columns of dataframe as per the schema of table in database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"jdbc:postgresql://127.0.0.1/etl\"\n",
    "properties = {\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"etl\",\n",
    "    \"password\": \"etl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the properties with the parameters required for jdbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SOGOSOV12AF72A285E|   ¿Dónde va Chichi?|ARGUVEV1187B98BA17|1997|313.12934|\n",
      "|SOMZWCG12A8C13C480|    I Didn't Mean To|ARD7TVE1187B99BFB1|   0|218.93179|\n",
      "|SOUPIRU12A6D4FA1E1| Der Kleine Dompfaff|ARJIE2Y1187B994AB7|   0|152.92036|\n",
      "|SOXVLOJ12AB0189215|     Amor De Cabaret|ARKRRTF1187B9984DA|   0|177.47546|\n",
      "|SOWTBJW12AC468AC6E|Broken-Down Merry...|ARQGYP71187FB44566|   0|151.84934|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_data_db = spark.read.jdbc(url=url,\\\n",
    "    table=\"songs\", \\\n",
    "    properties=properties)\n",
    "song_data_db.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+----+--------+\n",
      "|song_id|title|artist_id|year|duration|\n",
      "+-------+-----+---------+----+--------+\n",
      "+-------+-----+---------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_data_db.registerTempTable(\"songsdb\")\n",
    "song_data.registerTempTable(\"songsnew\")\n",
    "song_insert = spark.sql(\"select n.* from songsnew n left outer join songsdb d on d.song_id = n.song_id where d.song_id is null\")\n",
    "song_insert.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_insert.write.jdbc(url=url, table='songs',mode='append',properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process artist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|         artist_id|         artist_name|     artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|ARDR4AC1187FB371A1|Montserrat Caball...|                    |           null|            null|\n",
      "|AREBBGV1187FB523D2|Mike Jones (Featu...|         Houston, TX|           null|            null|\n",
      "|ARMAC4T1187FB3FA4C|The Dillinger Esc...|   Morris Plains, NJ|       40.82624|       -74.47995|\n",
      "|ARPBNLO1187FB3D52F|            Tiny Tim|        New York, NY|       40.71455|       -74.00712|\n",
      "|ARNF6401187FB57032|   Sophie B. Hawkins|New York, NY [Man...|       40.79086|       -73.96644|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artist_data = songdata[['artist_id','artist_name','artist_location', 'artist_latitude', 'artist_longitude']]\n",
    "artist_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+---------------+----------------+\n",
      "|artist_id|artist_name|artist_location|artist_latitude|artist_longitude|\n",
      "+---------+-----------+---------------+---------------+----------------+\n",
      "+---------+-----------+---------------+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artist_db = spark.read.jdbc(url=url,\\\n",
    "    table=\"artists\", \\\n",
    "    properties=properties)\n",
    "artist_db.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|         artist_id|         artist_name|     artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|AR9AWNF1187B9AB0B4|Kenny G featuring...|Seattle, Washingt...|           null|            null|\n",
      "|AR0IAWL1187B9A96D0|        Danilo Perez|              Panama|         8.4177|       -80.11278|\n",
      "|AR0RCMP1187FB3F427|    Billie Jo Spears|        Beaumont, TX|       30.08615|       -94.10158|\n",
      "|AREDL271187FB40F44|        Soul Mekanik|                    |           null|            null|\n",
      "|ARI3BMM1187FB4255E|        Alice Stuart|          Washington|        38.8991|         -77.029|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artist_data.registerTempTable(\"artist\")\n",
    "artist_db.registerTempTable(\"artist_db\")\n",
    "artist_insert = spark.sql(\"select distinct a.* from artist a left outer join artist_db d on a.artist_id= d.artist_id where d.artist_id is null\")\n",
    "artist_insert.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_insert.write.jdbc(url=url, table='artists',mode='append',properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+\n",
      "|     artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|           song|status|           ts|           userAgent|userId|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+\n",
      "|   Harmonia|Logged In|     Ryan|     M|            0|   Smith|655.77751| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|  Sehr kosmisch|   200|1542241826796|\"Mozilla/5.0 (X11...|    26|\n",
      "|The Prodigy|Logged In|     Ryan|     M|            1|   Smith|260.07465| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|The Big Gundown|   200|1542242481796|\"Mozilla/5.0 (X11...|    26|\n",
      "|      Train|Logged In|     Ryan|     M|            2|   Smith|205.45261| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|       Marry Me|   200|1542242741796|\"Mozilla/5.0 (X11...|    26|\n",
      "|Sony Wonder|Logged In|   Samuel|     M|            0|Gonzalez|218.06975| free|Houston-The Woodl...|   PUT|NextSong|1.540492941796E12|      597|      Blackbird|   200|1542253449796|\"Mozilla/5.0 (Mac...|    61|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logdata = logdata[logdata['page'] == 'NextSong']\n",
    "logdata.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---+----+-----+----+-------+\n",
      "|          start_time|hour|day|week|month|year|weekday|\n",
      "+--------------------+----+---+----+-----+----+-------+\n",
      "|50841-09-11 20:26...|  16| 14|  46|   11|2018|      2|\n",
      "|50841-09-19 10:23...|  16| 14|  46|   11|2018|      2|\n",
      "|50841-09-22 10:36...|  16| 14|  46|   11|2018|      2|\n",
      "|50842-01-24 08:03...|  19| 14|  46|   11|2018|      2|\n",
      "|50842-04-21 00:29...|  21| 14|  46|   11|2018|      2|\n",
      "+--------------------+----+---+----+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logdata.registerTempTable(\"log_data\")\n",
    "time_data = spark.sql(\"select FROM_UNIXTIME(ts,'yyyy-MM-dd HH:mm:ss.sss') as start_time,hour(from_unixtime(ts/1000)) as hour,day(from_unixtime(ts/1000)) as day,weekofyear(from_unixtime(ts/1000))  as week,month(from_unixtime(ts/1000)) as month,year(from_unixtime(ts/1000)) as year,weekday(from_unixtime(ts/1000)) as weekday from log_data\")\n",
    "time_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---+----+-----+----+-------+\n",
      "|start_time|hour|day|week|month|year|weekday|\n",
      "+----------+----+---+----+-----+----+-------+\n",
      "+----------+----+---+----+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_db = spark.read.jdbc(url=url,\\\n",
    "    table=\"time\", \\\n",
    "    properties=properties)\n",
    "time_db.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[start_time: string, hour: int, day: int, week: int, month: int, year: int, weekday: int]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_db.registerTempTable(\"timedb\")\n",
    "time_data.registerTempTable(\"time\")\n",
    "time_insert = spark.sql(\"select distinct t.* from time t left outer join timedb d on d.start_time = t.start_time where d.hour is null\")\n",
    "time_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_insert.write.jdbc(url=url, table='time',mode='append',properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, first_name: string, last_name: string, gender: string, level: string]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df = logdata[['userId', 'firstName', 'lastName', 'gender', 'level']]\n",
    "\n",
    "user_df= user_df.select(col(\"userId\").alias(\"user_id\").cast(IntegerType()),col(\"firstname\").alias(\"first_name\"),col(\"lastName\").alias(\"last_name\"),col(\"gender\"),col(\"level\"))\n",
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "+-------+----------+---------+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, first_name: string, last_name: string, gender: string, level: string]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_db = spark.read.jdbc(url=url,\\\n",
    "    table=\"users\", \\\n",
    "    properties=properties)\n",
    "user_db.show(5)\n",
    "user_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     85|   Kinsley|    Young|     F| paid|\n",
      "|     85|   Kinsley|    Young|     F| free|\n",
      "|     65|     Amiya| Davidson|     F| paid|\n",
      "|     53|   Celeste| Williams|     F| free|\n",
      "|     78|     Chloe|     Roth|     F| free|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.registerTempTable(\"users\")\n",
    "user_db.registerTempTable(\"users_db\")\n",
    "user_insert = spark.sql(\"select distinct u.* from users u left outer join users_db d on u.user_id= d.user_id where d.user_id is null\")\n",
    "user_insert.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_insert.write.jdbc(url=url, table='users',mode='append',properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
